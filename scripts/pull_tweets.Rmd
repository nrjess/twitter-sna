---
title: "Pulling Tweets"
author: "Nicole Jess"
date: "`r format(Sys.time(), '%Y-%m-%d, %H:%M:%S %Z')`"
output: 
  bookdown::html_document2:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding:  show
params: 
  outfile: "pull-tweets.html"
---

# Purpose
The script pulls Twitter data based on parameters defined by the user and saves a 
copy to an .RData file stored in the local repository where we can easily access 
it from other scripts.

```{r global-options, include=FALSE}
# Create a custom chunk hook/option for controlling font size in chunk & output.
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$cfsize != "normalsize", paste0("\n \\", options$cfsize,"\n\n", 
                                              x, "\n\n \\normalsize"), x)
  })

# Global chunk options (over-ridden by local chunk options)
knitr::opts_chunk$set(include  = TRUE, echo = TRUE, error = TRUE, 
                      message = FALSE, warning = FALSE, 
                      cfsize = "footnotesize", fig.height=8)

# Declare location of this script relative to the project root directory.
here::i_am(path = "scripts/pull_tweets.Rmd")
```

# Load R Packages
Load contributed R packages that we need to get additional functions. 

``` {r load-packages}
library(tidyverse)          # dplyr, ggplot2, tidyr, etc.
library(here)               # for here()
library(httr)               # to make HTTP requests to the Twitter API
library(jsonlite)           # to work with JSON object
library(knitr)              # for kable()
library(lubridate)          # for floor_date()
```


# Load data from previous pulls
This process is structured to be useful with a Twitter developer account with 
Basic level access. Currently, this gives the user access to data from Tweets posted 
in the last 7 days. So in order to gather data over a longer time interval, you may need 
to run this script multiple times over the course of several weeks. Note that you 
will get an error here if this is your first pull because there is no previous data to load
but you will be able to continue with the rest of the process described in this script.

``` {r load-data}
# Load data previously created by "scripts/pull_tweets.Rmd" if the data file exists

all_tweets <- NULL
load(file=here("data/all_tweets.RData"))

```


# Set up authentication
Authentication is set up using the bearer token that was saved in your .Renviron 
file in process described on our [README page](https://github.com/nrjess/twitter-sna/tree/main?tab=readme-ov-file#access-to-twitter-data).

```{r auth}

headers = c(`Authorization` = sprintf('Bearer %s', Sys.getenv("TWITTER_BEARER")))

```


# Define your query

Multiple keywords can be used as shown:

* query = "cat OR dog" returns tweets that contain the word "cat" or the word "dog"
* query = "cat dog" returns tweets that contain both words 

The query can also be restricted by users, exclude keywords, and filter by tweet 
types. Full list of options shown here: [Building Queries](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query)

``` {r define time intervals}


query = "((graduate OR grad) (student OR assistant OR worker) (union OR unionization OR wages OR healthcare OR NLRB OR labor OR contract OR strike))
         (is:retweet OR is:reply OR is:quote OR has:mentions)
         lang:en"

```

## Check tweet counts

The code below will allow you to check the number of tweets that will be captured
by the defined query. This will help you refine your query and understand the
amount of data a query will pull without counting toward your monthly Tweet cap.

```{r total-tweets}

params_count = list(`query` = query,
                    `granularity` = 'day')

count <- httr::GET(url = 'https://api.twitter.com/2/tweets/counts/recent',
                   httr::add_headers(.headers=headers),
                   query = params_count)

count_data <- httr::content(count,
                            as = 'parsed',
                            type = 'application/json',
                            simplifyDataFrame = TRUE) %>% pluck("data")

sum(count_data$tweet_count)

```

The total number of tweets pulled with this query is `r sum(count_data$tweet_count)`.

Figure \@ref(fig:timeseries-plot) shows the number of tweets that would be pulled 
plotted over time.

```{r timeseries-plot, fig.width=7, fig.height=4, fig.cap=FC}

# Figure caption
FC <- paste0("\\label{fig:timeseries-plot}",
            "Timeseries plot of tweets over time ")

ggplot(data = count_data,
       aes(x=as.Date(end), y=tweet_count, group=1)) +
  geom_line() +
  labs(x="Date", y="number of tweets") +
  scale_x_date(date_breaks = "1 day",
               date_labels = "%m-%d-%Y") +
  theme_classic()

```

## Pilot Pull

The code below will allow you to pull a small number of the tweets that will be 
captured by the defined query. Examining the text of the tweets will help you to
determine whether your query needs to be further refined to exclude irrelevant 
tweets or expanded to capture additional tweets that are not pulled by the current query.

Keep in mind that this will count toward your monthly tweet cap. We set this up 
to pull 50 tweets so that you can continue to make adjustments to the query and 
rerun this code until you settle on your final query without using up too much of 
your monthly tweet cap. You may choose to set the `max_results` field below to a 
different value to suit the needs of your project.

```{r pilot}

params_pull = list(`query` = query,
                   `max_results` = '50',
                   `tweet.fields` = 'author_id,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,public_metrics,text',
                   `user.fields` = 'created_at,description,entities,id,location,name,public_metrics,username,verified',
                   `expansions` = 'author_id,entities.mentions.username,in_reply_to_user_id,referenced_tweets.id.author_id',
                   `next_token` = NULL)

pull <- httr::GET(url = 'https://api.twitter.com/2/tweets/search/recent',
                  httr::add_headers(.headers = headers),
                  query = params_pull) %>%
  httr::content(as = 'parsed',
                type = 'application/json',
                simplifyDataFrame = TRUE)

pull_data <- pull %>% pluck("data") # extract data element to dataframe

```

```{r check pilot data}

head(pull_data)

```


# Pull tweets

This is the code that will pull the full data for tweets identified by your query. 
Be sure that you have tested your query by checking the tweet counts and conducting 
a pilot pull as described above to ensure that you are happy with your query before 
proceeding to this step as this may pull a large amount of data.


```{r tweet-pull}

# first page
params_pull = list(`query` = query,
                   `max_results` = '100', # max is 100, use pagination to pull larger numbers
                   `tweet.fields` = 'author_id,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,public_metrics,text',
                   `user.fields` = 'created_at,description,entities,id,location,name,public_metrics,username,verified',
                   `expansions` = 'author_id,entities.mentions.username,in_reply_to_user_id,referenced_tweets.id.author_id',
                   `next_token` = NULL)

pull <- httr::GET(url = 'https://api.twitter.com/2/tweets/search/recent',
                  httr::add_headers(.headers = headers),
                  query = params_pull) %>%
  httr::content(as = 'parsed',
                type = 'application/json',
                simplifyDataFrame = TRUE)

token <- pull$meta$next_token # save next token
pull_data <- pull %>% pluck("data") # extract data element to dataframe


# pagination
while(!is.null(token)){
 
  params_pull = list(`query` = query,
                   `max_results` = '100',
                   `tweet.fields` = 'author_id,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,public_metrics,text',
                   `user.fields` = 'created_at,description,entities,id,location,name,public_metrics,username,verified',
                   `expansions` = 'author_id,entities.mentions.username,in_reply_to_user_id,referenced_tweets.id.author_id',
                   `next_token` = token)
  
  new_pull <- httr::GET(url = 'https://api.twitter.com/2/tweets/search/recent',
                  httr::add_headers(.headers = headers),
                  query = params_pull) %>%
    httr::content(as = 'parsed',
                  type = 'application/json',
                  simplifyDataFrame = TRUE)
  
  token <- new_pull$meta$next_token # save next token
  new_pull_data <- new_pull %>% pluck("data") # extract data element to dataframe
  pull_data <- bind_rows(pull_data, new_pull_data) # merge with previous pages
}
  

```

# Merge new data with previous pull data

```{r tweet-merge}

all_tweets <- bind_rows(all_tweets, 
                        pull_data) %>%
  distinct()
  

```

# Save data

```{r save-data}
# here() creates a path relative to the location of the twitter-sna.proj file
# that will be portable across local repositories on different computers.

save(all_tweets, file=here(paste0("data/all_tweets.Rdata")))


```

